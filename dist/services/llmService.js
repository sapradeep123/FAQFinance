"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.llmService = void 0;
class LLMService {
    constructor() {
        this.mockProviders = [
            'OpenAI GPT-4',
            'Anthropic Claude',
            'Google Gemini',
            'Meta Llama',
            'Cohere Command'
        ];
        this.mockResponses = [
            "Based on the analysis of your question, here's a comprehensive response that takes into account multiple perspectives and current best practices.",
            "Let me break this down for you step by step. The key considerations are the context, requirements, and potential implications of your query.",
            "This is an interesting question that requires careful consideration. From my analysis, the most effective approach would be to consider the following factors.",
            "I've analyzed your question from multiple angles and can provide you with a detailed response that addresses the core issues you've raised.",
            "Your question touches on several important aspects. Let me provide you with a thorough analysis and actionable recommendations."
        ];
    }
    async ask({ threadId, question }) {
        await new Promise(resolve => setTimeout(resolve, 1500 + Math.random() * 2000));
        const ratings = this.mockProviders.map(provider => ({
            provider,
            score_percent: Math.floor(Math.random() * 30) + 70
        }));
        const avgScore = Math.round(ratings.reduce((sum, rating) => sum + rating.score_percent, 0) / ratings.length);
        const baseResponse = this.mockResponses[Math.floor(Math.random() * this.mockResponses.length)];
        const consolidatedAnswer = `${baseResponse}\n\nRegarding your question: "${question}"\n\nThis response has been generated by analyzing multiple AI models and consolidating their outputs for the most comprehensive answer. (Mock response for thread: ${threadId})`;
        return {
            consolidatedAnswer,
            ratings,
            avgScore
        };
    }
}
exports.llmService = new LLMService();
//# sourceMappingURL=llmService.js.map