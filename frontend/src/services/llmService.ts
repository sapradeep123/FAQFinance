import { API_BASE_URL } from '../config/clientEnv';

interface LLMRequest {
  threadId: string
  question: string
}

interface ProviderRating {
  provider: string
  score_percent: number
}

interface LLMResponse {
  consolidatedAnswer: string
  ratings: ProviderRating[]
  avgScore: number
}

class LLMService {
  private mockProviders = [
    'OpenAI GPT-4',
    'Anthropic Claude',
    'Google Gemini',
    'Meta Llama',
    'Cohere Command'
  ]

  private mockResponses = [
    "Based on the analysis of your question, here's a comprehensive response that takes into account multiple perspectives and current best practices.",
    "Let me break this down for you step by step. The key considerations are the context, requirements, and potential implications of your query.",
    "This is an interesting question that requires careful consideration. From my analysis, the most effective approach would be to consider the following factors.",
    "I've analyzed your question from multiple angles and can provide you with a detailed response that addresses the core issues you've raised.",
    "Your question touches on several important aspects. Let me provide you with a thorough analysis and actionable recommendations."
  ]

  async ask({ threadId, question }: LLMRequest): Promise<LLMResponse> {
    // TODO: Replace with actual API call to POST /chat/ask
    // const response = await fetch(`${API_BASE_URL}/chat/ask`, {
    //   method: 'POST',
    //   headers: {
    //     'Content-Type': 'application/json',
    //     'Authorization': `Bearer ${token}`
    //   },
    //   body: JSON.stringify({ threadId, question })
    // });
    // if (!response.ok) throw new Error('Failed to get LLM response');
    // return await response.json();
    
    // Simulate API delay
    await new Promise(resolve => setTimeout(resolve, 1500 + Math.random() * 2000))
    
    // Generate mock ratings for each provider
    const ratings: ProviderRating[] = this.mockProviders.map(provider => ({
      provider,
      score_percent: Math.floor(Math.random() * 30) + 70 // Random score between 70-100
    }))
    
    // Calculate average score
    const avgScore = Math.round(
      ratings.reduce((sum, rating) => sum + rating.score_percent, 0) / ratings.length
    )
    
    // Generate consolidated answer
    const baseResponse = this.mockResponses[Math.floor(Math.random() * this.mockResponses.length)]
    const consolidatedAnswer = `${baseResponse}\n\nRegarding your question: "${question}"\n\nThis response has been generated by analyzing multiple AI models and consolidating their outputs for the most comprehensive answer. (Mock response for thread: ${threadId})`
    
    return {
      consolidatedAnswer,
      ratings,
      avgScore
    }
  }
}

export const llmService = new LLMService()